# Tell hydra how to construct the config from sub confs by default
defaults:
  - model: funlib_unet
  - dataset: j0126
  - data_prep: default
  - _self_

# Common path prefix
path_prefix: ${oc.env:HOME}/lsdex

# Short description for manually tagging runs, to be set via hydra CLI
descr: ''
a: ${shortname}

# Generate random two-word codename using custom omgegaconf resolver
shortname: ${randomname:}${descr}

# Time stamp in ISO 8601-like format
# timestamp: ${now:%Y-%m-%d_%H-%M-%S}
timestamp: ${now:%m-%d_%H-%M}

# Version tag
v: v1

hydra:
  run:
    # Where to write logs and config copies
    dir: ${path_prefix}/${v}/${hydra.job.name}/${timestamp}_${shortname}

# Log config values with module logger
log_config: true

enable_zarr_results: false  # warning: concurrent writes possible
device: cuda


labels:
  aff:
    nhood: [[-1, 0, 0], [0, -1, 0], [0, 0, -1]]
  lsd:
    sigma: 120
    downsample: 2
    # sigma: 108
    # downsample: 4


wandb:
  ## These are directly passed to wandb.init() as **kwargs
  init_cfg:
    dir: ${path_prefix}/${v}/
    entity: mdth
    project: nseg
    name: ${shortname}
    tags:
      - mtlsd
  # Visualization options
  vis:
    enable_binary_labels: true
  # Log metrics per cube with filename suffix in addition to mean metrics
  enable_per_cube_metrics: true
  # Include only files with these extensions
  code_include_fn_exts: ['.py', '.json', '.yaml', '.txt']

training:
  iterations: 100001
  save_every: 250
  # save_every: 2
  # seed: 0  # TODO: Enable in training, make sure forked processes reseed!
  # num_workers: auto
  num_workers: 32
  # num_workers: 0

  # Main hyperparams

  # batch_size: 1
  # lr: 0.5e-4

  batch_size: 16
  lr: 1.0e-3

  save_jit: null


eval:
  cube_root: ${dataset.val_root}
  checkpoint_path: /u/mdraw/lsdex/v1/train_mtlsd/2023-04-14_04-02-04/model_checkpoint_20750_state_dict.pth
  result_zarr_root: ${path_prefix}/${v}/eval_zarr/
  threshold: 0.043
  fragment_threshold: 0.5

  write_groups:
    - raw0
    - gt_seg
    - pred_frag
    - pred_seg
